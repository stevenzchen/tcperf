{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensor_comprehensions as tc\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def matmul_grad(float(M,N) A, float(N,K), float(M,K) O_grad) -> (A_grad, B_grad){\\n  A_grad(i, j) +=! O_grad(i, kk) * B(j, kk)\\n  B_grad(i, j) +=! O_grad(kk, j) * A(kk, i)\\n}\\n',\n",
       " 'def convolution_strided_grad(float(N, C, H, W) I, float(M, C, KH, KW) W1, float(N, M, H, W) O_grad)\\n-> (I_grad, W1_grad) {{\\n  I_grad(n, c, h, w) +=! O_grad(n, m, {sh} * h - kh, {sw} * w - kw) * W1(m, c, kh, kw)\\n  W1_grad(m, c, kh, kw) +=! O_grad(n, m, {sh} * h - kh, {sw} * w - kw) * I(n, c, h, w)\\n}}\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as of current, there are only two layers with gradient code:\n",
    "# matmul and convolution_strided\n",
    "\n",
    "[tc.database[entry]['grad'] for entry in tc.database if 'grad' in tc.database[entry].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threads': 32}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default autotuner settings don't specify very much\n",
    "tc.autotuner_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as recommended by the authors for better performance\n",
    "tune_settings = {\n",
    "    \"threads\": 32,\n",
    "    \"generations\": 8,\n",
    "    \"pop_size\": 20,\n",
    "    \"number_elites\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tbmm_lang = \"\"\"\n",
    "    def tbmm(float(B, N, M) X, float(B, K, M) Y) -> (Z) {\n",
    "        Z(b, n, k) +=! X(b, n, m) * Y(b, k, m)\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Autotuning cache will be saved to: cache/tbmm_500_26_72_26.tc.cuda/options\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensor_comprehensions.mapping_options.Options at 0x7fd5cd8f7768>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune tbmm for the paper-given batch size of (B, N, M, K) = (500, 26, 72, 26), time how long it takes\n",
    "\n",
    "mat1 = torch.randn(500, 26, 72).cuda()\n",
    "mat2 = torch.randn(500, 26, 72).cuda()\n",
    "\n",
    "tbmm = tc.define(tbmm_lang, name='tbmm')\n",
    "tbmm.autotune(mat1, mat2, \n",
    "              cache='cache/tbmm_500_26_72_26.tc', \n",
    "              **tune_settings, \n",
    "              options=tc.Options('mlp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run twice to prep cuda\n",
    "\n",
    "tbmm(mat1, mat2, cache='cache/tbmm_500_26_72_26.tc')\n",
    "torch.cuda.synchronize()\n",
    "tbmm(mat1, mat2, cache='cache/tbmm_500_26_72_26.tc')\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 0.154343737001\n",
      "[ 0.00011347  0.00012295  0.00019211]\n"
     ]
    }
   ],
   "source": [
    "# test performance by running multiple iterations\n",
    "output = torch.zeros(500, 26, 26).cuda()\n",
    "\n",
    "timings = np.zeros(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    start = time.perf_counter()\n",
    "    tbmm(mat1, mat2, cache='cache/tbmm_500_26_72_26.tc', outputs=output)\n",
    "    torch.cuda.synchronize()\n",
    "    timings[i] = time.perf_counter() - start\n",
    "\n",
    "\n",
    "total_elapsed = np.sum(timings)\n",
    "print('total time: ' + str(total_elapsed))\n",
    "\n",
    "# get percentile statistics\n",
    "percentiles = np.percentile(timings, [0, 50, 90])\n",
    "print(percentiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 0.219243931997\n",
      "[ 0.00018492  0.00019968  0.00023665]\n"
     ]
    }
   ],
   "source": [
    "# time the torch equivalent\n",
    "\n",
    "timings = np.zeros(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    start = time.perf_counter()\n",
    "    output = mat1.matmul(mat2.permute(0, 2, 1))\n",
    "    torch.cuda.synchronize()\n",
    "    timings[i] = time.perf_counter() - start\n",
    "\n",
    "total_elapsed = np.sum(timings)\n",
    "print('total time: ' + str(total_elapsed))\n",
    "\n",
    "# get percentile statistics\n",
    "percentiles = np.percentile(timings, [0, 50, 90])\n",
    "print(percentiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Autotuning cache will be saved to: cache/tbmm_800_80_40_80.tc.cuda/options\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensor_comprehensions.mapping_options.Options at 0x7fd5cb94cbc8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune using a completely different size (B, N, M, K) = (800, 80, 40, 80), time how long it takes\n",
    "\n",
    "mat1 = torch.randn(500, 80, 40).cuda()\n",
    "mat2 = torch.randn(500, 80, 40).cuda()\n",
    "\n",
    "tbmm = tc.define(tbmm_lang, name='tbmm')\n",
    "tbmm.autotune(mat1, mat2, \n",
    "              cache='cache/tbmm_800_80_40_80.tc', \n",
    "              **tune_settings, \n",
    "              options=tc.Options('mlp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run twice to prep cuda\n",
    "\n",
    "tbmm(mat1, mat2, cache='cache/tbmm_800_80_40_80.tc')\n",
    "torch.cuda.synchronize()\n",
    "tbmm(mat1, mat2, cache='cache/tbmm_800_80_40_80.tc')\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 0.374608138006\n",
      "[ 0.00035463  0.00036372  0.00039369]\n"
     ]
    }
   ],
   "source": [
    "# test performance by running multiple iterations\n",
    "output = torch.zeros(800, 80, 80).cuda()\n",
    "\n",
    "timings = np.zeros(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    start = time.perf_counter()\n",
    "    tbmm(mat1, mat2, cache='cache/tbmm_800_80_40_80.tc', outputs=output)\n",
    "    torch.cuda.synchronize()\n",
    "    timings[i] = time.perf_counter() - start\n",
    "\n",
    "\n",
    "total_elapsed = np.sum(timings)\n",
    "print('total time: ' + str(total_elapsed))\n",
    "\n",
    "# get percentile statistics\n",
    "percentiles = np.percentile(timings, [0, 50, 90])\n",
    "print(percentiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 0.634489195998\n",
      "[ 0.00055517  0.00061861  0.00065862]\n"
     ]
    }
   ],
   "source": [
    "# time the torch equivalent\n",
    "\n",
    "timings = np.zeros(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    start = time.perf_counter()\n",
    "    output = mat1.matmul(mat2.permute(0, 2, 1))\n",
    "    torch.cuda.synchronize()\n",
    "    timings[i] = time.perf_counter() - start\n",
    "\n",
    "total_elapsed = np.sum(timings)\n",
    "print('total time: ' + str(total_elapsed))\n",
    "\n",
    "# get percentile statistics\n",
    "percentiles = np.percentile(timings, [0, 50, 90])\n",
    "print(percentiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Autotuning cache will be saved to: cache/tbmm_200_200_200_200.tc.cuda/options\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensor_comprehensions.mapping_options.Options at 0x7fd5cb94c9d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune using a completely different size (B, N, M, K) = (200, 200, 200, 200), time how long it takes\n",
    "\n",
    "mat1 = torch.randn(200, 200, 200).cuda()\n",
    "mat2 = torch.randn(200, 200, 200).cuda()\n",
    "\n",
    "tbmm = tc.define(tbmm_lang, name='tbmm')\n",
    "tbmm.autotune(mat1, mat2, \n",
    "              cache='cache/tbmm_200_200_200_200.tc', \n",
    "              **tune_settings, \n",
    "              options=tc.Options('mlp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run twice to prep cuda\n",
    "\n",
    "tbmm(mat1, mat2, cache='cache/tbmm_200_200_200_200.tc')\n",
    "torch.cuda.synchronize()\n",
    "tbmm(mat1, mat2, cache='cache/tbmm_200_200_200_200.tc')\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 17.621973661\n",
      "[ 0.0173515   0.01744545  0.01764447]\n"
     ]
    }
   ],
   "source": [
    "# test performance by running multiple iterations\n",
    "output = torch.zeros(200, 200, 200).cuda()\n",
    "\n",
    "timings = np.zeros(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    start = time.perf_counter()\n",
    "    tbmm(mat1, mat2, cache='cache/tbmm_200_200_200_200.tc', outputs=output)\n",
    "    torch.cuda.synchronize()\n",
    "    timings[i] = time.perf_counter() - start\n",
    "\n",
    "\n",
    "total_elapsed = np.sum(timings)\n",
    "print('total time: ' + str(total_elapsed))\n",
    "\n",
    "# get percentile statistics\n",
    "percentiles = np.percentile(timings, [0, 50, 90])\n",
    "print(percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 2.37877126001\n",
      "[ 0.00209192  0.00235626  0.00250093]\n"
     ]
    }
   ],
   "source": [
    "# time the torch equivalent\n",
    "\n",
    "timings = np.zeros(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    start = time.perf_counter()\n",
    "    output = mat1.matmul(mat2.permute(0, 2, 1))\n",
    "    torch.cuda.synchronize()\n",
    "    timings[i] = time.perf_counter() - start\n",
    "\n",
    "total_elapsed = np.sum(timings)\n",
    "print('total time: ' + str(total_elapsed))\n",
    "\n",
    "# get percentile statistics\n",
    "percentiles = np.percentile(timings, [0, 50, 90])\n",
    "print(percentiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
